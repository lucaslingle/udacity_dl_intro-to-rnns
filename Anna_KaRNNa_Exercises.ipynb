{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, we'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 64, 57, 72, 76, 61, 74,  1, 16,  0,  0,  0, 36, 57, 72, 72, 81,\n",
       "        1, 62, 57, 69, 65, 68, 65, 61, 75,  1, 57, 74, 61,  1, 57, 68, 68,\n",
       "        1, 57, 68, 65, 67, 61, 26,  1, 61, 78, 61, 74, 81,  1, 77, 70, 64,\n",
       "       57, 72, 72, 81,  1, 62, 57, 69, 65, 68, 81,  1, 65, 75,  1, 77, 70,\n",
       "       64, 57, 72, 72, 81,  1, 65, 70,  1, 65, 76, 75,  1, 71, 79, 70,  0,\n",
       "       79, 57, 81, 13,  0,  0, 33, 78, 61, 74, 81, 76, 64, 65, 70], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "Here is where we'll make our mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "We have our text encoded as integers as one long array in `encoded`. Let's create a function that will give us an iterator for our batches. I like using [generator functions](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/) to do this. Then we can pass `encoded` into this function and get our batch generator.\n",
    "\n",
    "The first thing we need to do is discard some of the text so we only have completely full batches. Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences) and $M$ is the number of steps. Then, to get the number of batches we can make from some array `arr`, you divide the length of `arr` by the batch size. Once you know the number of batches and the batch size, you can get the total number of characters to keep.\n",
    "\n",
    "After that, we need to split `arr` into $N$ sequences. You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences (`n_seqs` below), let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$ where $K$ is the number of batches.\n",
    "\n",
    "Now that we have this array, we can iterate through it to get our batches. The idea is each batch is a $N \\times M$ window on the array. For each subsequent batch, the window moves over by `n_steps`. We also want to create both the input and target arrays. Remember that the targets are the inputs shifted over one character. You'll usually see the first input character used as the last target character, so something like this:\n",
    "```python\n",
    "y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "```\n",
    "where `x` is the input batch and `y` is the target batch.\n",
    "\n",
    "The way I like to do this window is use `range` to take steps of size `n_steps` from $0$ to `arr.shape[1]`, the total number of steps in each sequence. That way, the integers you get from `range` always point to the start of a batch, and each window is `n_steps` wide.\n",
    "\n",
    "> **Exercise:** Write the code for creating batches in the function below. The exercises in this notebook _will not be easy_. I've provided a notebook with solutions alongside this notebook. If you get stuck, checkout the solutions. The most important thing is that you don't copy and paste the code into here, **type out the solution code yourself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr) // characters_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:(n_batches * characters_per_batch)]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:(n+n_steps)]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1] = x[:, 1:]\n",
    "        y[:, -1] = x[:, 0]  \n",
    "        # solution says we can use first character of every mini-sequence as terminating char, \n",
    "        # but I am not sure why it's ok.\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1985223,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1985223,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_batches shouldn't mess up the shape of our original array 'encoded'. sanity check:\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[31 64 57 72 76 61 74  1 16  0]\n",
      " [ 1 57 69  1 70 71 76  1 63 71]\n",
      " [78 65 70 13  0  0  3 53 61 75]\n",
      " [70  1 60 77 74 65 70 63  1 64]\n",
      " [ 1 65 76  1 65 75 11  1 75 65]\n",
      " [ 1 37 76  1 79 57 75  0 71 70]\n",
      " [64 61 70  1 59 71 69 61  1 62]\n",
      " [26  1 58 77 76  1 70 71 79  1]\n",
      " [76  1 65 75 70  7 76 13  1 48]\n",
      " [ 1 75 57 65 60  1 76 71  1 64]]\n",
      "\n",
      "y\n",
      " [[64 57 72 76 61 74  1 16  0  0]\n",
      " [57 69  1 70 71 76  1 63 71 65]\n",
      " [65 70 13  0  0  3 53 61 75 11]\n",
      " [ 1 60 77 74 65 70 63  1 64 65]\n",
      " [65 76  1 65 75 11  1 75 65 74]\n",
      " [37 76  1 79 57 75  0 71 70 68]\n",
      " [61 70  1 59 71 69 61  1 62 71]\n",
      " [ 1 58 77 76  1 70 71 79  1 75]\n",
      " [ 1 65 75 70  7 76 13  1 48 64]\n",
      " [75 57 65 60  1 76 71  1 64 61]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you implemented `get_batches` correctly, the above output should look something like \n",
    "```\n",
    "x\n",
    " [[55 63 69 22  6 76 45  5 16 35]\n",
    " [ 5 69  1  5 12 52  6  5 56 52]\n",
    " [48 29 12 61 35 35  8 64 76 78]\n",
    " [12  5 24 39 45 29 12 56  5 63]\n",
    " [ 5 29  6  5 29 78 28  5 78 29]\n",
    " [ 5 13  6  5 36 69 78 35 52 12]\n",
    " [63 76 12  5 18 52  1 76  5 58]\n",
    " [34  5 73 39  6  5 12 52 36  5]\n",
    " [ 6  5 29 78 12 79  6 61  5 59]\n",
    " [ 5 78 69 29 24  5  6 52  5 63]]\n",
    "\n",
    "y\n",
    " [[63 69 22  6 76 45  5 16 35 35]\n",
    " [69  1  5 12 52  6  5 56 52 29]\n",
    " [29 12 61 35 35  8 64 76 78 28]\n",
    " [ 5 24 39 45 29 12 56  5 63 29]\n",
    " [29  6  5 29 78 28  5 78 29 45]\n",
    " [13  6  5 36 69 78 35 52 12 43]\n",
    " [76 12  5 18 52  1 76  5 58 52]\n",
    " [ 5 73 39  6  5 12 52 36  5 78]\n",
    " [ 5 29 78 12 79  6 61  5 59 63]\n",
    " [78 69 29 24  5  6 52  5 63 76]]\n",
    " ```\n",
    " although the exact numbers will be different. Check to make sure the data is shifted over one step for `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building the model\n",
    "\n",
    "Below is where you'll build the network. We'll break it up into parts so it's easier to reason about each bit. Then we can connect them up into the whole network.\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "First off we'll create our input placeholders. As usual we need placeholders for the training data and the targets. We'll also create a placeholder for dropout layers called `keep_prob`. This will be a scalar, that is a 0-D tensor. To make a scalar, you create a placeholder without giving it a size.\n",
    "\n",
    "> **Exercise:** Create the input placeholders in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, shape=(batch_size,num_steps))\n",
    "    targets = tf.placeholder(tf.int32, shape=(batch_size,num_steps))\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### LSTM Cell\n",
    "\n",
    "Here we will create the LSTM cell we'll use in the hidden layer. We'll use this cell as a building block for the RNN. So we aren't actually defining the RNN here, just the type of cell we'll use in the hidden layer.\n",
    "\n",
    "We first create a basic LSTM cell with\n",
    "\n",
    "```python\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "```\n",
    "\n",
    "where `num_units` is the number of units in the hidden layers in the cell. Then we can add dropout by wrapping it with \n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "```\n",
    "You pass in a cell and it will automatically add dropout to the inputs or outputs. Finally, we can stack up the LSTM cells into layers with [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell). With this, you pass in a list of cells and it will send the output of one cell into the next cell. Previously with TensorFlow 1.0, you could do this\n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.MultiRNNCell([cell]*num_layers)\n",
    "```\n",
    "\n",
    "This might look a little weird if you know Python well because this will create a list of the same `cell` object. However, TensorFlow 1.0 will create different weight matrices for all `cell` objects. But, starting with TensorFlow 1.1 you actually need to create new cell objects in the list. To get it to work in TensorFlow 1.1, it should look like\n",
    "\n",
    "```python\n",
    "def build_cell(num_units, keep_prob):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    return drop\n",
    "    \n",
    "tf.contrib.rnn.MultiRNNCell([build_cell(num_units, keep_prob) for _ in range(num_layers)])\n",
    "```\n",
    "\n",
    "Even though this is actually multiple LSTM cells stacked on each other, you can treat the multiple layers as one cell.\n",
    "\n",
    "We also need to create an initial cell state of all zeros. This can be done like so\n",
    "\n",
    "```python\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "```\n",
    "\n",
    "Below, we implement the `build_lstm` function to create these LSTM cells and the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_cell(lstm_size, keep_prob):\n",
    "    ### Build the LSTM Cell\n",
    "    \n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    # Add dropout to the cell outputs\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    return drop\n",
    "\n",
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size,keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### RNN Output\n",
    "\n",
    "Here we'll create the output layer. We need to connect the output of the RNN cells to a full connected layer with a softmax output. The softmax output gives us a probability distribution we can use to predict the next character, so we want this layer to have size $C$, the number of classes/characters we have in our text.\n",
    "\n",
    "If our input has batch size $N$, number of steps $M$, and the hidden layer has $L$ hidden units, then the output is a 3D tensor with size $N \\times M \\times L$. The output of each LSTM cell has size $L$, we have $M$ of them, one for each sequence step, and we have $N$ sequences. So the total size is $N \\times M \\times L$. \n",
    "\n",
    "We are using the same fully connected layer, the same weights, for each of the outputs. Then, to make things easier, we should reshape the outputs into a 2D tensor with shape $(M * N) \\times L$. That is, one row for each sequence and step, where the values of each row are the output from the LSTM cells. We get the LSTM output as a list, `lstm_output`. First we need to concatenate this whole list into one array with [`tf.concat`](https://www.tensorflow.org/api_docs/python/tf/concat). Then, reshape it (with `tf.reshape`) to size $(M * N) \\times L$.\n",
    "\n",
    "One we have the outputs reshaped, we can do the matrix multiplication with the weights. We need to wrap the weight and bias variables in a variable scope with `tf.variable_scope(scope_name)` because there are weights being created in the LSTM cells. TensorFlow will throw an error if the weights created here have the same names as the weights created in the LSTM cells, which they will be default. To avoid this, we wrap the variables in a variable scope so we can give them unique names.\n",
    "\n",
    "> **Exercise:** Implement the output layer in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # Concatenate lstm_output over axis 1 (the columns)\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(seq_output, shape=(-1, in_size))\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                (in_size, out_size), \n",
    "                mean=0.0, \n",
    "                stddev=(1.0 / np.sqrt(out_size))\n",
    "            )\n",
    "        )\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.add(tf.matmul(x, softmax_w), softmax_b)\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training loss\n",
    "\n",
    "Next up is the training loss. We get the logits and targets and calculate the softmax cross-entropy loss. First we need to one-hot encode the targets, we're getting them as encoded characters. Then, reshape the one-hot targets so it's a 2D tensor with size $(M*N) \\times C$ where $C$ is the number of classes/characters we have. Remember that we reshaped the LSTM outputs and ran them through a fully connected layer with $C$ units. So our logits will also have size $(M*N) \\times C$.\n",
    "\n",
    "Then we run the logits and targets through `tf.nn.softmax_cross_entropy_with_logits` and find the mean to get the loss.\n",
    "\n",
    ">**Exercise:** Implement the loss calculation in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per sequence per step\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    cross_entropy_terms = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=y_reshaped, \n",
    "        logits=logits)\n",
    "    loss = tf.reduce_mean(cross_entropy_terms)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Optimizer\n",
    "\n",
    "Here we build the optimizer. Normal RNNs have have issues gradients exploding and disappearing. LSTMs fix the disappearance problem, but the gradients can still grow without bound. To fix this, we can clip the gradients above some threshold. That is, if a gradient is larger than that threshold, we set it to the threshold. This will ensure the gradients never grow overly large. Then we use an AdamOptimizer for the learning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the network\n",
    "\n",
    "Now we can put all the pieces together and build a class for the network. To actually run data through the LSTM cells, we will use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn). This function will pass the hidden and cell states across LSTM cells appropriately for us. It returns the outputs for each LSTM cell at each step for each sequence in the mini-batch. It also gives us the final LSTM state. We want to save this state as `final_state` so we can pass it to the first LSTM cell in the the next mini-batch run. For `tf.nn.dynamic_rnn`, we pass in the cell and initial state we get from `build_lstm`, as well as our input sequences. Also, we need to one-hot encode the inputs before going into the RNN. \n",
    "\n",
    "> **Exercise:** Use the functions you've implemented previously and `tf.nn.dynamic_rnn` to build the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN with tf.nn.dynamic_rnn \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here are the hyperparameters for the network.\n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to copy it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Time for training\n",
    "\n",
    "This is typical training code, passing inputs and targets into the network, then running the optimizer. Here we also get back the final LSTM state for the mini-batch. Then, we pass that state back into the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}.ckpt`\n",
    "\n",
    "> **Exercise:** Set the hyperparameters above to train the network. Watch the training loss, it should be consistently dropping. Also, I highly advise running this on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 3960...  Training loss: 1.2068...  0.4763 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            display.clear_output(wait=True)\n",
    "            \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_d{}.ckpt\".format(counter, lstm_size, num_layers))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}_d{}.ckpt\".format(counter, lstm_size, num_layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i3960_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3200_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3600_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3800_l512_d2.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3960_l512_d2.ckpt\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \", n=5):\n",
    "    samples = [c for c in prime]\n",
    "    \n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True) \n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        \n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab), n)\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i3960_l512_d2.ckpt'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_name:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/Adam_1\n",
      "[  2.98701752e-09   1.12818794e-08   3.49353768e-09 ...,   2.89313040e-09\n",
      "   2.45372478e-09   2.58292898e-09]\n",
      "tensor_name:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights\n",
      "[[ 0.06996887  0.11649907  0.18184812 ...,  0.26486248 -0.08052508\n",
      "  -0.1132154 ]\n",
      " [-0.01522851  0.04797746 -0.11067834 ...,  0.07246272  0.26834911\n",
      "   0.05272453]\n",
      " [ 0.30675411 -0.13460676  0.27785024 ...,  0.37532464 -0.05215592\n",
      "  -0.10368945]\n",
      " ..., \n",
      " [-0.21007143 -0.01034986  0.00191667 ...,  0.18761837 -0.03268498\n",
      "  -0.15263623]\n",
      " [-0.08051489  0.26401338 -0.26916745 ..., -0.01368065 -0.28915605\n",
      "  -0.23917878]\n",
      " [ 0.06516229 -0.56173038 -0.09796987 ..., -0.1699965  -0.01801279\n",
      "   0.04343418]]\n",
      "tensor_name:  softmax/Variable_1/Adam\n",
      "[ -7.00336386e-05   1.47326544e-04   5.58796601e-05   3.78345751e-04\n",
      "   2.26013640e-06   2.99536305e-06   2.78044513e-06   1.52085515e-04\n",
      "  -9.17639991e-05  -5.69082149e-05   8.29187502e-06   3.51653493e-04\n",
      "  -8.97173377e-05   2.68355798e-04   8.73241333e-06   6.56636348e-06\n",
      "   1.88728518e-05   5.21498396e-06  -2.38225111e-05  -5.60340004e-06\n",
      "   9.63578714e-07   5.67155666e-06   7.60074045e-06  -5.81439144e-06\n",
      "   9.71902773e-06   2.97368370e-05   1.00234975e-04   6.55994663e-05\n",
      "   2.49337813e-06  -1.62703127e-05   3.03109809e-05  -4.98771442e-05\n",
      "   4.23215970e-05   7.01785684e-05   1.22405718e-05  -9.62217528e-05\n",
      "  -5.66869312e-05   1.57606730e-04   7.45778925e-06   7.49900719e-05\n",
      "  -1.38309460e-05  -4.60191859e-06   4.33468267e-05   6.47655816e-06\n",
      "  -3.38679856e-05   7.64818742e-06   5.41132613e-05  -1.35865324e-04\n",
      "  -6.61524828e-05  -3.18591560e-06   1.04905499e-04   4.44411999e-05\n",
      "   1.72537966e-06   6.21477739e-05  -3.24274743e-06   3.19688916e-05\n",
      "  -6.28179805e-06  -5.15496882e-04  -4.57958267e-05   1.28626460e-04\n",
      "  -1.22903992e-04  -1.18156371e-03   1.66503698e-04   5.26454860e-05\n",
      "   2.87155999e-04  -4.74608416e-04   1.14119857e-05   2.67172669e-04\n",
      "  -2.70015764e-04  -4.09642744e-05   4.15018818e-04  -4.48331615e-04\n",
      "   2.13396692e-04   3.45081971e-05   2.17712630e-04  -2.39357076e-04\n",
      "  -7.21562013e-04   8.30531571e-05   2.25253520e-04   2.13852312e-04\n",
      "   4.05821447e-05   9.19793019e-05   5.82214780e-05]\n",
      "tensor_name:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights\n",
      "[[ 0.09734391  0.25844485  0.31119329 ...,  0.20449868 -0.33650407\n",
      "  -0.04551681]\n",
      " [ 0.04741105 -0.08234551  0.32911751 ..., -0.24034557  0.16125952\n",
      "   0.03300229]\n",
      " [ 0.02488804  0.26081988  0.12856746 ..., -0.45454255  0.11402002\n",
      "   0.1218224 ]\n",
      " ..., \n",
      " [-0.12106187 -0.22405107  0.1162793  ...,  0.12347575  0.03228715\n",
      "   0.04487169]\n",
      " [-0.11714341 -0.03612259  0.24045452 ..., -0.0040126   0.04261462\n",
      "   0.01851455]\n",
      " [ 0.01177639 -0.17217377 -0.11691529 ..., -0.05199907  0.04952683\n",
      "   0.12767506]]\n",
      "tensor_name:  beta1_power\n",
      "0.0\n",
      "tensor_name:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/Adam\n",
      "[[ -5.46535048e-08  -1.08701806e-05  -2.58577484e-06 ...,  -3.20231862e-07\n",
      "    8.65558320e-07  -2.89293752e-07]\n",
      " [  8.84476322e-07   8.69328323e-06   1.87597125e-05 ...,  -3.42485964e-06\n",
      "    9.34187881e-07  -4.31055241e-06]\n",
      " [  6.77422065e-08   4.80723543e-07   3.76380996e-07 ...,  -7.54873355e-08\n",
      "   -4.72332800e-07  -3.38475928e-07]\n",
      " ..., \n",
      " [  2.47944308e-06   1.72944192e-06  -2.25566100e-06 ...,   3.41446139e-06\n",
      "    1.68358360e-06   1.36911672e-06]\n",
      " [  1.98282760e-06   5.85443409e-08  -6.41979932e-06 ...,   5.51957373e-06\n",
      "    2.62739263e-06   1.10389374e-06]\n",
      " [  2.71580234e-06  -2.79923444e-08  -1.44335161e-06 ...,   2.15483738e-06\n",
      "    1.13991086e-06  -2.14700162e-06]]\n",
      "tensor_name:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/Adam_1\n",
      "[[  1.18245788e-10   3.61369018e-10   2.26542451e-10 ...,   3.69856673e-10\n",
      "    5.72985270e-10   3.93630517e-10]\n",
      " [  1.42315285e-10   4.45005949e-10   2.10223602e-10 ...,   4.17662849e-10\n",
      "    6.70463352e-10   4.72849815e-10]\n",
      " [  2.58986110e-10   6.60681232e-10   1.98468006e-10 ...,   4.42017617e-10\n",
      "    9.35827416e-10   4.10089046e-10]\n",
      " ..., \n",
      " [  6.44740483e-10   7.19660997e-10   2.36244940e-10 ...,   4.90061380e-10\n",
      "    1.20046295e-09   5.05968878e-10]\n",
      " [  1.38026501e-09   2.70369749e-09   1.34402578e-09 ...,   2.30815389e-09\n",
      "    3.67309094e-09   2.17957608e-09]\n",
      " [  4.35217057e-10   8.92879437e-10   3.87680943e-10 ...,   1.06309994e-09\n",
      "    1.64508862e-09   1.75973347e-09]]\n",
      "tensor_name:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/Adam\n",
      "[  1.22718734e-06  -7.66283677e-07   2.40934060e-05 ...,   3.67041775e-06\n",
      "  -1.63215973e-05  -1.68525112e-05]\n",
      "tensor_name:  softmax/Variable/Adam_1\n",
      "[[  2.14223462e-07   1.32028993e-06   2.41826612e-08 ...,   6.58557875e-09\n",
      "    2.32324595e-07   9.26111543e-09]\n",
      " [  8.09791345e-07   1.79241545e-06   1.78368094e-08 ...,   2.11892566e-08\n",
      "    1.30637218e-07   6.50878640e-09]\n",
      " [  3.32818445e-07   1.69850398e-06   2.91356059e-08 ...,   2.95714813e-08\n",
      "    1.76034419e-07   1.43227910e-08]\n",
      " ..., \n",
      " [  2.53758486e-07   9.04493618e-07   1.62147540e-08 ...,   1.60286684e-09\n",
      "    4.92470029e-08   2.38560705e-09]\n",
      " [  8.36733705e-07   2.27382702e-06   2.62692215e-08 ...,   4.17133883e-08\n",
      "    3.09871439e-07   9.15689835e-09]\n",
      " [  1.23587270e-07   7.93881668e-07   1.06284652e-08 ...,   1.30894460e-08\n",
      "    1.74342574e-07   6.33361941e-09]]\n",
      "tensor_name:  beta2_power\n",
      "0.0190074\n",
      "tensor_name:  softmax/Variable_1\n",
      "[-0.16135305  0.32566091 -0.4421995  -0.23425223 -0.13927849 -0.14854161\n",
      " -0.15018226 -0.44995514 -0.22938706 -0.28608164 -0.15458652 -0.14379361\n",
      " -0.33224958 -0.20866875 -0.14599985 -0.12599091 -0.11126794 -0.1243458\n",
      " -0.12580465 -0.13773096 -0.13500418 -0.13555455 -0.13006534 -0.14039432\n",
      " -0.1297666  -0.37247217 -0.52580231 -0.49448523 -0.14290681 -0.077971\n",
      " -0.13842224 -0.21527502 -0.19745487 -0.17858204 -0.18461382 -0.19856152\n",
      " -0.15667395 -0.04491798 -0.16831288 -0.25284243 -0.25387534 -0.2360061\n",
      " -0.16477315 -0.17042404 -0.19354391 -0.14156155 -0.21457589 -0.10753652\n",
      " -0.0590497  -0.14406495 -0.26174787 -0.11740764 -0.14310123 -0.14871664\n",
      " -0.14012815 -0.23427324 -0.14352708  0.25517648 -0.0884691   0.03447005\n",
      "  0.01401125  0.17882487 -0.09580129 -0.18596767  0.12224065  0.0967112\n",
      " -0.3547411  -0.40333575  0.03143888 -0.02469533  0.19764766  0.11558155\n",
      " -0.14238369 -0.49463928  0.04330409  0.23305802  0.3343035  -0.21404289\n",
      " -0.25427988  0.03719077 -0.39439794 -0.26047519 -0.38792238]\n",
      "tensor_name:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases\n",
      "[ 0.05367617 -0.02700207  0.12371756 ...,  0.10397816  0.20310418\n",
      "  0.1344178 ]\n",
      "tensor_name:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/Adam\n",
      "[  3.31394222e-05   5.19528658e-06  -3.05296339e-06 ...,  -3.46096349e-05\n",
      "   5.66852941e-06   5.29213821e-06]\n",
      "tensor_name:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/Adam_1\n",
      "[[  1.80836721e-10   9.85365123e-10   8.14258660e-11 ...,   1.76438461e-10\n",
      "    4.69643456e-11   8.93481192e-11]\n",
      " [  7.30443095e-10   1.32268418e-09   7.97765687e-10 ...,   1.59285474e-10\n",
      "    6.19546969e-10   2.88616603e-10]\n",
      " [  6.46974282e-13   2.69607860e-12   2.73259692e-12 ...,   7.30132155e-13\n",
      "    6.41067711e-13   1.74914011e-12]\n",
      " ..., \n",
      " [  4.60323203e-11   1.15494003e-10   5.98225913e-11 ...,   9.29043370e-11\n",
      "    3.21085034e-11   5.67837444e-11]\n",
      " [  8.40564562e-11   1.47929863e-10   9.80955733e-11 ...,   7.87093862e-11\n",
      "    5.81414500e-11   7.34528688e-11]\n",
      " [  7.69557473e-11   2.04904010e-10   7.91163385e-11 ...,   1.11899066e-10\n",
      "    6.83639881e-11   7.78937886e-11]]\n",
      "tensor_name:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/Adam\n",
      "[[ -1.71836575e-06   7.13178565e-08   4.07666721e-06 ...,   8.39012785e-07\n",
      "   -6.00913040e-07  -6.65284460e-07]\n",
      " [  3.98541397e-06   3.69658142e-06   8.33007198e-06 ...,  -4.61961690e-06\n",
      "    1.20898449e-05   1.72300372e-06]\n",
      " [ -3.61374055e-06   7.69168946e-06  -5.79118114e-06 ...,   3.89839533e-06\n",
      "    1.80982752e-07  -2.99737781e-06]\n",
      " ..., \n",
      " [ -8.00858015e-06   8.77520870e-06  -1.27324108e-06 ...,   3.13980308e-06\n",
      "   -3.25014548e-06  -3.68302472e-06]\n",
      " [  1.03010516e-05   1.28606907e-05  -4.14519718e-06 ...,  -2.19913982e-05\n",
      "    2.75895582e-06   1.14513095e-05]\n",
      " [ -5.64663424e-06   7.75065746e-06   5.26302347e-06 ...,   1.02175318e-06\n",
      "   -1.99014403e-06   9.46605155e-07]]\n",
      "tensor_name:  softmax/Variable_1/Adam_1\n",
      "[  7.69678354e-06   2.07787452e-05   1.21433132e-07   5.22200139e-07\n",
      "   9.96623672e-09   1.16176002e-08   8.41676417e-09   3.72390389e-07\n",
      "   2.23791741e-08   1.87379730e-08   1.10708722e-08   1.89804916e-06\n",
      "   1.80448936e-07   1.58291414e-06   1.19628725e-08   9.57601909e-09\n",
      "   2.18407941e-08   1.20702763e-08   1.28596218e-08   1.20373933e-08\n",
      "   9.26200983e-09   1.13850351e-08   1.29217748e-08   1.21270567e-08\n",
      "   1.27340476e-08   3.48299949e-08   1.15382718e-07   1.50473696e-07\n",
      "   1.08792442e-08   2.22693586e-07   8.34300380e-08   4.28087432e-08\n",
      "   5.66778695e-08   3.13951354e-08   3.03252889e-08   4.16733528e-08\n",
      "   9.15729856e-08   3.25145493e-07   1.41942857e-08   7.75394540e-08\n",
      "   1.08477167e-07   5.36925100e-08   5.42879697e-08   5.81846749e-08\n",
      "   4.72163002e-08   1.05061018e-08   2.88847239e-08   1.45794047e-07\n",
      "   1.41824700e-07   1.58029110e-08   7.52725526e-08   9.65998908e-08\n",
      "   1.56366546e-08   6.66638513e-08   1.26020963e-08   5.20303871e-08\n",
      "   9.23146626e-09   6.96423285e-06   1.01187243e-06   1.40235909e-06\n",
      "   1.75967421e-06   9.03342243e-06   1.22213737e-06   8.56989118e-07\n",
      "   4.94993674e-06   5.88568491e-06   6.78271164e-08   3.56521582e-07\n",
      "   2.49786126e-06   1.91252184e-06   4.17938372e-06   5.20857839e-06\n",
      "   1.08228687e-06   7.99861226e-08   3.23531663e-06   4.37369044e-06\n",
      "   7.07091385e-06   1.68750705e-06   6.33655532e-07   1.61773903e-06\n",
      "   1.10403384e-07   1.03168929e-06   4.38150884e-08]\n",
      "tensor_name:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/Adam_1\n",
      "[  7.81098564e-09   1.22449002e-08   5.15334708e-09 ...,   1.48759085e-08\n",
      "   2.25063026e-08   1.26987452e-08]\n",
      "tensor_name:  softmax/Variable/Adam\n",
      "[[ -1.56783819e-04   2.94408033e-04   9.16478439e-06 ...,   2.47175758e-05\n",
      "   -4.88578953e-05  -2.29644793e-05]\n",
      " [ -3.22653577e-05  -1.98475784e-04  -6.03471835e-06 ...,  -4.94555798e-06\n",
      "    3.69265072e-05   4.89735612e-06]\n",
      " [  7.84196163e-05  -4.96911234e-04  -6.00774638e-06 ...,  -2.91136457e-05\n",
      "   -2.16433458e-04  -7.72313979e-06]\n",
      " ..., \n",
      " [ -1.55145855e-04   5.11483922e-05  -1.78371738e-06 ...,  -1.10022183e-05\n",
      "    1.55899415e-05  -2.54140946e-06]\n",
      " [ -8.88263730e-06   1.58510506e-04  -3.61421335e-05 ...,  -2.33958053e-05\n",
      "    2.11977560e-04  -8.75593014e-06]\n",
      " [ -5.16560831e-05  -1.76065500e-04  -4.69388033e-06 ...,  -6.07917682e-05\n",
      "    3.28044334e-05   1.62677097e-07]]\n",
      "tensor_name:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases\n",
      "[ 0.02228881  0.02769415  0.05214708 ..., -0.01029713  0.0171712\n",
      " -0.02689827]\n",
      "tensor_name:  softmax/Variable\n",
      "[[ 0.03087653  0.01390373  0.10472757 ..., -0.22824542 -0.0098831\n",
      "  -0.14472456]\n",
      " [ 0.2019427   0.21604164  0.22958799 ..., -0.00523298  0.02129203\n",
      "   0.02246535]\n",
      " [ 0.05827349  0.03067854 -0.01315107 ...,  0.09694079 -0.03910337\n",
      "  -0.04976871]\n",
      " ..., \n",
      " [-0.10612378 -0.13616018 -0.07486954 ...,  0.26437664  0.05760672\n",
      "   0.1633427 ]\n",
      " [ 0.09160373  0.06153235  0.11588273 ..., -0.07172799 -0.00766956\n",
      "  -0.00894399]\n",
      " [-0.01329216 -0.00764661  0.0970557  ..., -0.01787951  0.08202565\n",
      "  -0.04734543]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "print_tensors_in_checkpoint_file(file_name=tf.train.latest_checkpoint('checkpoints'), tensor_name='', all_tensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fartian one who saw\n",
      "a man an influence to any hanrs, and as she could not set to the cares\n",
      "and altalking, which was a capal of her husband and to and wonder.\n",
      "\n",
      "\"I arrived, better not see.\"\n",
      "\n",
      "\"Yes, by his own death. I don't know that in the morning,\n",
      "and that it would be drearing,\" she thought, \"the cheek to say to\n",
      "them, but if they're beating in it all to the provincial country.\"\n",
      "\n",
      "\"Oh, what has the countess? If you know why it's a letter to discus it\n",
      "will the bear to say to.\"\n",
      "\n",
      "\"Yes, and I could disappoint you and said.\"\n",
      "\n",
      "\"You should be in them. If it was a lottle to my servecces, and I have sat\n",
      "down to the same, but the desire it's not for you. Yes; but there was\n",
      "no luncting their singers.\"\n",
      "\n",
      "\"Only this you're so awful! His sister all so much a fearful from the\n",
      "country. How imagined it was anything that tears askates her that I was so\n",
      "mictrificully before all the teachness in his opinion, and all the same\n",
      "world, the country and househ, and then, why shall be to be seen to be\n",
      "strengthed from him. How did not answer, but which is no doctor, what\n",
      "should have been to greenel to the mental particular times of a thing or\n",
      "the materois, that in the man as the service that has been better for\n",
      "me.\"\n",
      "\n",
      "\"Any one censt to say.\"\n",
      "\n",
      "\"I'm not going to see her.\n",
      "\n",
      "\"Well, tellime her all, I didn't say what I should be an inspance,\" said\n",
      "Sergey Ivanovitch.\n",
      "\n",
      "\"In the provincial deact, why?\"\n",
      "\n",
      "\"Yes, that's so wanted to.\"\n",
      "\n",
      "He went on. This convictions with her shade he had said, and thought\n",
      "and sort from subfried.\n",
      "\n",
      "\"In the door and then so?\"\n",
      "\n",
      "\"Yes, but this worse in the same thing,\" shouted her. \"There were no one\n",
      "thinking of it, because I should go on, and I was to get the servants\n",
      "about him,\" said Stepan Arkadyevitch.\n",
      "\n",
      "\"That'll be about a men who has been to do. Well, that's not a garden of\n",
      "me to morrow,\" he said.\n",
      "\n",
      "\"I don't like it.... Well, have I had as at once married about them\n",
      "without a struck me had been intrisalf, but he would be to go away. I\n",
      "can't help their cort about his more word that I ca\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farr afet hes thers and sot al sath tho sho cimesens, he sas tond. I tin that ans he thim had the wars ot an ton sas than aret and hard won he her and ant he tishe alsatid the sadile sose thon has areras oud to the here as of him he and ant te he wothe sand,. As ad avin tha sil he whens thit the\n",
      "shot an was he sint af the corte har wast the hin ho the serensing on than her astit the sat if sar tast hit he\n",
      "ponse whas as on attheres ang had ser to whet ar ot hored the wothe that, wad ho shathe sores afitig hard ond tit he tin the we he her sat he sis oud\n",
      "ans ansensint on hom whe so he wing at ande so fiting the thar tho he whing he tosent wha to toth, whit ho thin the sis him seras hans hesd ang thos the\n",
      "pad soste wore sont won the talt on whe he son thers this ale he whr sins hed tor thet thit thet he was the thith ses ond and he cime an shand wos the hat she sosen the hesese the\n",
      "sering of has alensing th mon himer, thes hus ther we tans ofrer thand the core to man th the the shan ther som \n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i200_l512_d2.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farlings\n",
      "antwernt the prone the were some to his his, woo disent to have had allided the could his had she he said, but his and this tomis and thought his with his which as\n",
      "herped her that, he can trought his a merticaly and\n",
      "with so that with the with so dister tarking the\n",
      "comsint was her has sand the suppersticined has and she with a could bet ansertite into the seading and and the\n",
      "marie and\n",
      "him to\n",
      "her aspined time and tran alled were hourd to she mad\n",
      "time this he doult on tho\n",
      "grothant him as that he with her had the mars, who chad so manding\n",
      "was stidly hus\n",
      "that he was not in she had\n",
      "not is himping,\n",
      "and her suppiseded to\n",
      "comtrion, still to the conciturt and were to\n",
      "her\n",
      "said and heress of\n",
      "all that so dowire hould and her his was ald still have, and the sumated ot the\n",
      "room on\n",
      "this heres, to alling had strook of him.\n",
      "\n",
      "The his shis on thein woll to thim him.\n",
      "\n",
      "Alexey Alexandrovitch she tome the comenter he seact harding, had a late him\n",
      "her wouth a dount the counder onter on the promered\n",
      "wat an\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i600_l512_d2.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arkady\n",
      "had not always all her sont. Stepan Arkadyevitch went a talking was this so thinking as something one and\n",
      "that he had been said to say\n",
      "the point of a look and the chetter, and sooned the pleaser, and serenes, and world him\n",
      "the sense of the streeng at his wife ald so minute as\n",
      "the cried and the stort the\n",
      "change to have been to be to the hers, and all the corvants to spile of the\n",
      "plower,\n",
      "how he crassing her feeling. And trutching the stard of the compleaner something wite with a succroms to take into his hands of the poorter to any he had been\n",
      "trouble the close, and the contiction of a supprision of the servion,\" he said.\n",
      "\n",
      "At their said of the servic of sectirity and he setited\n",
      "that the prince was saying with him.\n",
      "\n",
      "\"What as a saw, that I'll make a good about him.\"\n",
      "\n",
      "\"I's the man about a latter about\n",
      "it. I will to get. I deat the side of the same, and as stood with the word\n",
      "tryes, be and moshoring official in the maderal and always.\n",
      "\n",
      "At all the seeming then a caller seem, the parter with h\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i1200_l512_d2.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Arkady\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arkady's\n",
      "friend. Anna said. She did not know how she had brought a longer\n",
      "and been ten of the supper to hear, but he was always staying to all\n",
      "sometimes that they were talking of her husband. Anna drew her\n",
      "early for him far into the porter he heard a country, to take him\n",
      "with the clarks of the book, he seated to all the change of the same\n",
      "tires, and the point in three detation of the compention of the wine\n",
      "of the same, who was no mistaken as he did anything at all in any\n",
      "conscience at at once the conversation was nothing, and that when the\n",
      "place in this temper and to began to go astachs to the chair.\n",
      "\n",
      "She was still to care, but to brill to brought an entrance.\n",
      "\n",
      "\"This work, think on the thing I would be some more and delighted in\n",
      "him; and I shall have a short of thir to make them along.\"\n",
      "\n",
      "\"What a pictitity?\" said Anna, as though as too, she had said the sire\n",
      "song which had say an idea, should be in some other province that\n",
      "the closing the princess were to stop in the face with the man which\n",
      "wa\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i3960_l512_d2.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Arkady\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arkady\n",
      "Parlov troubled it walted and driven. He could not have busy\n",
      "crossed, had all she wanted.\n",
      "\n",
      "\n",
      "\n",
      "Chapter 32\n",
      "\n",
      "\n",
      "When she sat day at having abroad in the people of these peasants, when he\n",
      "saw the carpenter, they, taking it to her and take all the train of\n",
      "this strange creet, that his brows and his blothes and a position was\n",
      "that he had a subject, who had bad happened,\" thought Levin, which all\n",
      "the sofal sense of home he had been said; but the strange world they\n",
      "had been darked off it, and the contlasion of all he had never told\n",
      "him to say.\n",
      "\n",
      "She had bored in his hoste at the possible thought in them and a shrill\n",
      "princess, he had been in the soul of the sound that had been already\n",
      "about it.\n",
      "\n",
      "\"You do nothing as an inveridulted men with yourself, I can do asliev, I\n",
      "say she has been inference,\" he said, smaking to him the strange forest\n",
      "with her eyes.\n",
      "\n",
      "\"This is the manner of the children are the faces of a conversation,\n",
      "but I'm very indifferent, but I'm not as to see her to think this way.\"\n",
      "\n",
      "The m\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i3960_l512_d2.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(sorted(vocab)), prime=\"Arkady\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arkady Dvorkovich.\n",
      "\n",
      "The praice had not alone with her, the feelings that she had been done what\n",
      "he could not be desired, he forgotten a children without a least as he\n",
      "would say, and that she came to the conversation, and he saw the must have\n",
      "any soul, and that he had been desprised that their sente of the pleasure\n",
      "he had never told me about the provinces. He had to choose their\n",
      "sentement when the prince they were said: \n",
      "\"No. You meant; but I shall have been a significance. I should be\n",
      "nurse her and said, and this is not only to say in this sight....\"\n",
      "\n",
      "At the train of the crowd of his wife throwed the tears of the\n",
      "conversation with him, and so he had not conved her face was so stopped.\n",
      "He forgod that she had not said at that singer. And, after heart and\n",
      "satisfaction, and a five clear and tone in the carriage, the cruminess\n",
      "of his sorres had broken her husband's hones in the country, who was\n",
      "the tried--that was that he does not see to the confise as she has\n",
      "brought a country waiting about this tense, who \n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i3960_l512_d2.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(sorted(vocab)), prime=\"Arkady Dvorkovich\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flynnas, was strayge\n",
      "out of the corner into the conversation. She did not conceive it\n",
      "her feeling of tender had thinked and should all think than enjoyment to\n",
      "be answered with the memory of all the work that in answer is so strayge,\n",
      "and that she was to do anything was to be a principle to see twan ever\n",
      "in the care where he was to say to the table. Anna said, taking her\n",
      "high spare at the steps with his sense from the chind, they came\n",
      "out to the straight, some position he said.\n",
      "\n",
      "\"I didn't care, I will be done any one of this tooth it,\" said Stepan\n",
      "Arkadyevitch, attaining his servant, and he see himself in the creations\n",
      "of her eyes that there was no one was stronger than any harrows any\n",
      "considerable parcy. He will gave him the steps of her life was a moment\n",
      "and some man as she could not see her. The prince's sound of a feeling\n",
      "of their means with the present that is already always barelings out\n",
      "of his sense of theight. And this were she was not to say. He could\n",
      "have been taking her fine state about the stets of a little baby to his\n",
      "secretary, and at his frown, and, and her life was a luncy on his face, and\n",
      "sat down the character with a started face that had been said to him,\n",
      "what he had been in the carriage.\n",
      "\n",
      "The positions of the werts--how this happies of the sight of the maintares\n",
      "was a second annay for a low of the person, and the peasants and them,\n",
      "as they said to him that they were not simply that this carriage, at that\n",
      "steps too have not called her. She was strange, but was in his sort,\n",
      "that he had been saying the striking of the position. The side, and who\n",
      "was simple and that he came about himself, and the father had been sat\n",
      "simply telling him and attoneseless the same thing his feet, were for and\n",
      "taken alone, and she had thought it with the considerations.\n",
      "\n",
      "Alexey Alexandrovitch said at that temperare he was saying, and that the\n",
      "position of her lovery stopped had been in the conversation.\n",
      "\n",
      "\"The door what was the continual part, and this any anterest the taking of\n",
      "th\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Flynn\", n=8)\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna Arkadyevna\n",
      "had been in the matcer, and he was streaming and any weariny of\n",
      "their conversation. But a man of the composure, and to him, and he heard\n",
      "the children, and he was sitting in the character of the presence, where\n",
      "they answered that in hardly an intense fach, she had talked of them. But\n",
      "she could not choose the crowde offer her to see this trivally,\n",
      "and has an expression of subject than had come burden out of the\n",
      "party. But when he could not see him with a smile of satisfaction,\n",
      "bull in what was in society. And that he had not tried to be in the means\n",
      "of this soul. The place he saw the service what something she was still\n",
      "more that he was as that she seets on her son, he stolded his hand, and\n",
      "his high raps, and turning away into her former, he shouted in the\n",
      "sone that had been asked for the paint. He heard the priest saw that\n",
      "she were set a letter and at the sight of the senses. And with her head,\n",
      "and the memery, she had been angry.\"\n",
      "\n",
      "\"I shall be so saying a little with her. What do you've come to see how had\n",
      "been all the memories of my confrience about the same, to take the\n",
      "same time in the same and mere though all all the certain position wihh\n",
      "what it was one with the principle.\"\n",
      "\n",
      "\"They could never have been stirling to you...\"\n",
      "\n",
      "\"Well, I'm not the strough, this would be the same assimps of my contrary,\n",
      "the boy which was something absorbange. I shall see a collecting or time to\n",
      "come to much. I've been talking to her as they were staying at the midd\n",
      "at home...\"\n",
      "\n",
      "\"Yes, they're alone with you to the creature of you.\"\n",
      "\n",
      "\"Yes, that's that?\" said the same significance of his string. \"They have\n",
      "no met of the mairs who had so coming to see this, and that you are all\n",
      "at all, and how she was seen.\n",
      "\n",
      "\n",
      "Those consciousness of the characters in his wife was that so money they\n",
      "sent to her.\n",
      "\n",
      "And these sirples the province was saw that a moment he was desired. He\n",
      "warted almost to contrall with the priving chollet of the tone as\n",
      "the country. The most lively that the manshart of\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Anna\", n=8)\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Ritzen was\n",
      "comprehensible to him, and which would have been, to hear him in\n",
      "a little thought how she went away from this wife indeed, than the other\n",
      "house he was not stoping. He had seen her husband, she shoted his\n",
      "suppirt as it was the conversation, which should be delighted in, who\n",
      "had been so much sensible for his word is an offer. The princess and the\n",
      "marshal of tendes he did not love him. He saw that the position of the\n",
      "craps his beard had brought them. The continual chance, her eyes told\n",
      "him and always despribed, and turned away to his strong, silence, almost\n",
      "as he cared to come out.\n",
      "\n",
      "\"What do you tell her. As is she would be all happy in a solection.\" And\n",
      "he had said that in his external time that he could not be set for him to\n",
      "drave to the carriage and talked of the solution of the children,\n",
      "so she was told him to stand, was a concert of sorries of soces,\n",
      "was not introduced about him, began all the same time, stepsing at\n",
      "the same, the confined she was a grief who was asking that they was a\n",
      "shame of his wife. Telling the confusion of the conversation, she\n",
      "sat a minute, with a change out in his shart and the prove the beart\n",
      "of his household with the preposs that at all he had been a great deal of\n",
      "any state. She would arrange any doubt of taking the carriage with the\n",
      "children and a man shooting at him that he could not care to be threw things\n",
      "and days by their conversation. See her handsome face were a shade\n",
      "in her first, though he were so standing the first moment and how as the\n",
      "old carry had a lade that she had been satisfferted at him, as it was\n",
      "the carriage with her son and he come to her. He saw something to\n",
      "stand at him. At the table when she would say to them as he had to shoot.\n",
      "\n",
      "\"And I were all about that.\"\n",
      "\n",
      "\"What am I to say about me to be desired to make her mean with her, and\n",
      "that I shall give me they will see them. What are you always do the\n",
      "passed been and took another! He has nothing to say; but if he would give\n",
      "him all them on the subject of her faces\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"The Ritz\", n=8)\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locking had said.\n",
      "\n",
      "\"All I stall at the previous door of their conversation.\"\n",
      "\n",
      "\"I am going away, you were all so more of my path.\"\n",
      "\n",
      "\"Where are you going?\" she said, like a sense of thought of something,\n",
      "to say why the share hundred of his sense interested him. She went out\n",
      "of the rest, but at the strained hat at a law of his soficiam horses she\n",
      "was said, saying have been a sign for that moment, he sent on his hair\n",
      "as he could not be an answer. She did not, without spail in the side\n",
      "shoule taking the birch second coacents to the servant, and, as though\n",
      "he stupid her things of his his forese intented and he had never taken\n",
      "in their present to the country, and while she could not see whom\n",
      "he went on with the fearful than he was too, which had been talking\n",
      "to him; but he could not have liked to say should not hear the cares\n",
      "and the children and him.\n",
      "\n",
      "Anna was that at the priest of the train and the pass, sitting out, he\n",
      "had said:\n",
      "\n",
      "\"I don't listen to my hand and when I had been ill, brought to you, and\n",
      "how did the distinct of all those worked, but this was she was a gentlemen. Bry\n",
      "they have nothing been all at once always find to this force, and he can't\n",
      "but, they dive this sister's sorrop of the satisfaction in which the\n",
      "districtly was the point at his wife.\"\n",
      "\n",
      "\"No, I consider it, that' waiting it!\"\n",
      "\n",
      "\"Yes; but I can't say it's only one to me.\"\n",
      "\n",
      "She thought that it was so sorting in his wife. He crued his brother and\n",
      "her head, and the marshal of suddenly had not become her and was so\n",
      "steal of some chance at the service in the country in a continual smile,\n",
      "which she felt all something she was so seeing with a single time at once\n",
      "of the same tears of any other peculiarly arrove her hair as it can\n",
      "say to make a meaning on that man, and the still servants as he his\n",
      "feeling of any hundred of the servant's sale had been satisfied. All the\n",
      "same thing she had always been and to say it. She saw that they would have\n",
      "stayed by the principle to her, and the countess at the merry of\n",
      "his own\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Lock\", n=8)\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorosthel and the memoliee of twity of the matter in the province.\n",
      "\n",
      "That in the forms as though happy for her. The more holses would be to\n",
      "but her his, the soul that the solitiou and mire, the playful his orrivility\n",
      "of the same against they was that it was instead of his brother, the\n",
      "marshal than the capital comparison, and so interested, the concert the\n",
      "such assembire with something such contraly, as he was to spok, as they\n",
      "were a long south face and sone of station, at all the pointing was the\n",
      "same answers. The first she was, at a still and of a man who had\n",
      "been done and had never seen in all the carriage, he could be dinner, and\n",
      "which would have said the case of the praything the peasants had not been\n",
      "sure to bring in.\n",
      " Anna had struggled their porter to his brow, as he was talking\n",
      "of a sort of life.\n",
      "\n",
      "\"The merchings were the children, what was tired?\"\n",
      "\n",
      "\"I ask the memories of this first and anyone.\"\n",
      "\n",
      "\"I wanted to say, but when this, that I had said to you; that's a\n",
      "minute, and I'd better tried to mind able to see you.\"\n",
      "\n",
      "\"I don't know what we was sitting in. They were a muddle of a single.\"\n",
      "Alexey Alexandrovitch was all the thought of his forgettings, the\n",
      "supiriral stayes and hands of his wife, she wretee to any often the stand\n",
      "of the sound of the subject, without wheak she was all seemed\n",
      "to her, and he was so fold what he would have loved, and had been at to\n",
      "their provinces and he was saying. Alexey Alexandrovitch was saying when\n",
      "he did not said that he said it. She saw a minute to br offered the country\n",
      "husband. But the stands were taking her side, and all that she was\n",
      "still an inder freedom who had been disappained by her fashion.\n",
      "\n",
      "\"What do you talk,\" said the same time, sitting out of his brother's and\n",
      "a setical, and the sight of the first thing that her hands were attacking\n",
      "a least. Tending his hand, and he was saying. He stopped her so at\n",
      "tones that the marshal of the words were always been better as a single\n",
      "subjuct. She wanted in strock her. The conversation as t\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Soros\", n=8)\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Killary!\n",
      "\n",
      "She had stopped so much to be sure to give him her husband, and had\n",
      "consinted her.\n",
      "\n",
      "He choused that she was not so terrible to she was simply because he was\n",
      "an action again.\n",
      "\n",
      "They went up to his fingers, which shrigged him, an instant\n",
      "sening to shut her.\n",
      "\n",
      "That indeplation was the same the children and the bed of this action\n",
      "when he had still seen her, had alranded, and that he was all about the\n",
      "recaltial strangers, when he should have sent her all that had carried\n",
      "towards the thanger. The samaly shame, and his face were sotting over\n",
      "his brother of the service, seeing these portraits where he sat something\n",
      "will be at once always to say, and to say it with her secrets, as though\n",
      "to dream it were sitting over two of a country at a prince, he saw at the\n",
      "same tender. He was already and started in her feelings in the\n",
      "subject, he forgot whech he was already and was not too. At the stable\n",
      "always dare and strunger that having seemed to have said that he came\n",
      "over the communication with her soul, and that he was already short it\n",
      "out that the same women went on.\n",
      "\n",
      "Seeing her son to talk to him, her shoulders the call--all the comparison\n",
      "of the way, and that he was in her short heavy, and so the morning with\n",
      "a sight of straightered the sen and sat still sitting at the stemn on\n",
      "the relations of the sentement who was a minute that husben on her eyes, and\n",
      "he had sent himself, and he was saying at the prince. The study he\n",
      "had been talked with someone that with his sister, he felt three\n",
      "such a leash on the tears, and as though so that she came to him.\n",
      "\n",
      "\"What am I to be taste, I say. When I had supposed that you do you know it.\"\n",
      "\n",
      "\"What a long ago and help make or?\" he said, she said.\n",
      "\n",
      "\"Yes,\" had sat down at the familiar stigures. \"We've said it in your face,\"\n",
      "Anna said; \"I shall never come, and say that it's so as the matter of many of\n",
      "the minutes, but they don't know, what I have been, he say that he's a sente\n",
      "of allost the stream.\n",
      "\n",
      "\"I don't know what's the fair, that it's not a go\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Killary\", n=8)\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wtfor her sentary said how it was something was so as to take about a change\n",
      "to be asking,\" said Stepan Arkadyevitch, waiting for his heart, and\n",
      "tood his shirt will that struck her, and the peasant came out there was\n",
      "the prince, she had been so left, as that in the mother and his brether\n",
      "he could not say to how at anyway.\n",
      "\n",
      "\"And I could say that you are not a singing than he's to consert. It was\n",
      "something she could not have said whether he walked to the teacher and\n",
      "his wife's answer, ther as the conversation to her heart, they have\n",
      "to see the same thing, to tear the party as the corner and woman in\n",
      "the same time in spite of as though they had been too and allowed to bring\n",
      "him. They't heary that has thought of a sint, that she had been trying\n",
      "to stop the chain. She did not come to the tears.\n",
      "\n",
      "\"That was a little face. He can't think of your husband with him, but it\n",
      "could say that your serfords. There's all her mother in a shade of\n",
      "such as they he has a long been to distrable impossible and was\n",
      "in the party.\"\n",
      "\n",
      "And to be sure to say, when he had already had to dinner the moment of his\n",
      "face, and showed that their marriages was that the peasant wished out,\n",
      "but she were as though in the persuation after his father and husband\n",
      "had not, but they were talking to him, and that he had already been\n",
      "answered, he felt all his brought to their creside, the bed word to\n",
      "be in a little at ant all and should have been despused to see her with\n",
      "the marshal of the passionate tender. And the sisters were as this\n",
      "terrible\n",
      "thrown in silence, which he could not say that they did not care him,\n",
      "and when he was an interest in the morning at once or despair, and when\n",
      "Alexey Alexandrovitch wrote to see him. And she had been seen and so\n",
      "long, as he had troubled it, and that it was this way out of his wife's\n",
      "cource she had no divorce, but he went up to the stars, all with his book,\n",
      "streck on the possibility of his hap and training about the propirity\n",
      "of a complaints to her to the place. But she was so th\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Wtf\", n=8)\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
